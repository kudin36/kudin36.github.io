<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />

    <title>KY Data Science Portfolio</title>
<!--
DREAM PULSE
https://templatemo.com/tm-536-dream-pulse
-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,600" />
    <link rel="stylesheet" href="css/all.min.css" />
    <link rel="stylesheet" href="css/bootstrap.min.css" />
    <link rel="stylesheet" href="slick/slick.css"/>    
    <link rel="stylesheet" href="slick/slick-theme.css"/>    
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/templatemo-dream-pulse.css" />
  </head>
  <body>
    <main class="container-fluid">
      <div class="row">        
          <nav id="tmSidebar" class="tm-bg-black-transparent tm-sidebar">
            <button class="navbar-toggler" type="button" aria-label="Toggle navigation">
              <i class="fas fa-bars"></i>
            </button>
            <div class="tm-sidebar-sticky">
<br>
              <ul id="tmMainNav" class="nav flex-column text-uppercase text-right tm-main-nav">
                <li class="nav-item">
                  <a href="#intro" class="nav-link active">
                    <span class="d-inline-block mr-3">Intro</span>
                    <span class="d-inline-block tm-white-rect"></span>
                  </a>
                </li>
                <li class="nav-item">
                  <a href="#portfolio1" class="nav-link">
                    <span class="d-inline-block mr-3">Exploratory<br>Data<br>Analysis</span>
                    <span class="d-inline-block tm-white-rect"></span>
                  </a>
                </li>
                <li class="nav-item">
                  <a href="#portfolio2" class="nav-link">
                    <span class="d-inline-block mr-3">Word Clouds</span>
                    <span class="d-inline-block tm-white-rect"></span>
                  </a>
                </li>
                <li class="nav-item">
                  <a href="#portfolio3" class="nav-link">
                    <span class="d-inline-block mr-3">Machine <br> Learning</span>
                    <span class="d-inline-block tm-white-rect"></span>
                  </a>
                </li>
              </li>
              <li class="nav-item">
                <a href="#portfolio4" class="nav-link">
                  <span class="d-inline-block mr-3">Sentiment</span>
                  <span class="d-inline-block tm-white-rect"></span>
                </a>
              </li>
              <li class="nav-item">
                <a href="#portfolio5" class="nav-link">
                  <span class="d-inline-block mr-3">Dashboard</span>
                  <span class="d-inline-block tm-white-rect"></span>
                </a>
              </li>
                <li class="nav-item">
                  <a href="#about" class="nav-link">
                    <span class="d-inline-block mr-3">About Me</span>
                    <span class="d-inline-block tm-white-rect"></span>
                  </a>
                </li>
              </ul>
              <ul class="nav flex-row tm-social-links">
                <li class="nav-item">
                  <a href="https://www.facebook.com/kudin36" class="nav-link tm-social-link"target="_blank">
                    <i class="fab fa-facebook-f"></i>
                  </a>
                </li>
                <li class="nav-item">
                  <a href="https://www.linkedin.com/in/khairudinyaacob" class="nav-link tm-social-link" target="_blank">
                    <i class="fab fa-linkedin-in"></i>
                  </a>
                </li>
              </a>
            </li>
          </li>
          <li class="nav-item">
            <a href="mailto:kudin36@yahoo.com"class="nav-link tm-social-link" target="_blank">
              <i class="fab fa-yahoo"></i>
            </a>
          </li>
        </a>
      </li>
            <li class="nav-item">
              <a href="https://github.com/kudin36" class="nav-link tm-social-link" target="_blank">
                <i class="fab fa-github"></i>
              </a>
            </li>
              </ul>
            </div>
          </nav>
          
          <main role="main" class="ml-sm-auto col-12">
            <div
              class="parallax-window"
              data-parallax="scroll"
              data-image-src="img/headerdatasciencejpg.jpg">
              <div class="tm-section-wrap">
                <section id="intro" class="tm-section">
                    <div class="tm-bg-white-transparent tm-intro">
                        <h2 class="tm-section-title mb-5 text-uppercase tm-color-primary"style="font-size:70px">Data Science Portfolio by<br>&nbsp &nbsp &nbsp &nbsp Khairudin Yaacob</h2>
                        <p class="tm-color-gray">
                        </p>
                        <p class="mb-0 tm-color-gray">
                        </p>
                    </div>              
                </section>
            </div>            
          </div>


                            <div class="tm-section-wrap bg-white">
                            <section id="portfolio1" class="row tm-section">
                            <div class="col-12 tm-section-pad">
                            <div class="tm-flex-item-left">
                            <div class="tm-w-80">
                            <h2 class="tm-color-primary tm-section-title mb-4">Exploratory Data Analysis & Visualization</h2>
                            <p class="mb-5"><img src="img/linephone.png" alt="telco" style="float:left;width:80px;height:80px;">
                            <p style="color: #069"; fontsize:10> Telco</p> <br>
                            <h1 style="color:black; font-size:30px;">Using Data Analytic Techniques To Visualize Telco Churn</h1><br>
                            The datasets and complete code can be found at the link below.<br>
                            Dataset link: <a href="https://raw.githubusercontent.com/theleadio/datascience_demo/master/telco_customer_churn_dataset.csv" target="_blank">https://raw.githubusercontent.com/telco_customer_churn_dataset.csv</a><br>
                            Project link: <a href="https://colab.research.google.com/drive/1RZ8pzXqTyz7YOFr2nWi8fswV_pmmzHiQ?usp=sharing" target="_blank">https://colab.research.google.com/drive/1RZ8pzXqTyz7YOFr2nWi8fswV_pmmzHiQ?usp=sharing</a><br><br>
                            The method I am using to analyze the dataset is by using Data Science OSEMN framework. It can be best explained by this 
                            article writen by Dr. Lau <a href="https://towardsdatascience.com/5-steps-of-a-data-science-project-lifecycle-26c50372b492" target="_blank">here</a>.<br>
                            <img src="img/OSEMN.png" alt="telco">  
                            <br><br>
                            <u><b>Importing Python Libraries</b></u><br>
                            First, we need to import the necessary python libraries for this analysis. These include the basic packages such 
                            pandas, seaborn and matplotlib.
                            
                            <div class="transparentbox">
                              <p>import pandas as pd<br>
                                import seaborn as sns<br>
                                import matplotlib.pyplot as plt</p>
                            </div><br><br>
                            <u><b>Data Source</b></u><br>
                            The data that I am using for this portfolio is the raw dataset sample given in the class as per link dataset before. 
                            In a quick glance it contains columns and rows with mostly words & numbers in the table.
                            I will use Python to perform all data analysis and visualization. This data is originally in a CSV file and we will 
                            convert it into a data frame by using Pandas.<br>
                            <div class="transparentbox">
                             <p>df = pd.read_csv('https://raw.githubusercontent.com/theleadio/datascience_demo/
                               <br>master/telco_customer_churn_dataset.csv')</p>
                            </div>
                            Let’s take a look at what we have.
                            <div class="transparentbox">
                              <p>df.sample(5)</p>
                            </div>
                            <img src="img/sample.png" alt="sam" style="width:1000px"> 
                            <img src="img/sample1.png" alt="sam1"style="width:1000px"> 
                            Our focus is the ‘Churn’ column which contains the sample of subscriber churning or not.<br><br>
                            <u><b>Data Scrubbing</b></u><br>
                            Next, we will check if the columns have any null values.
                            <div class="transparentbox">
                              <p>df.isnull().sum()</p>
                            </div>
                            <img src="img/null.png" alt="null"><br>
                            The important thing is we do not have or minimal null values in the columns, so we can proceed with other cleaning.<br> 
                            Next we look data type of each value in the columns
                            <div class="transparentbox">
                              <p>df.info()</p>
                            </div>
                            <img src="img/info.png" alt="info"><br>
                            Looking at each 20 column value data are object input except for 'SeniorCitizen', 'tenure' & 'MonthlyCharges' columns.
                            For 'customerID' columns since that data type object is unique representing each subscriber, we do not need to clean this column and leave as it is.
                            Accurate analyzing, require data to be in either integer or float which we will do.<br>
                            Let check the rest of the 17 columns input data, and see if we can change it from object to numerical.
                            We group the columns as col tor easier call up and use the function unique to check.
                            <div class="transparentbox">
                              <p>col = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 
                                'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'TotalCharges', 'Churn']
                                <br>
                              for column in col:
                              print(column, ':', df[column].unique(), '\n'))</p>
                            </div>
                            Let see the result.<br>
                            <img src="img/unique.png" alt="unique"><br>
                            There is a mixed input of data inside the table. We try to clean from left to right column and group it as much as we can for conversion/mapping in a simpler coding. 
                            First column 'gender'', we can convert Female, Male data object to numerical 0 & 1 respectively using mappping function as per below.
                            <div class="transparentbox">
                              <p>df['gender'] = df['gender'].map({"Female":0, "Male":1})</p>
                            </div>
                            Next one is 'Partner' columns which have NO & Yes input, with 3 other columns as well.
                            We group it under binary_columns and map it No, Yes to 0,1 respectively
                            <div class="transparentbox">
                              <p>binary_columns = ["Partner", "PhoneService", "Dependents", "PaperlessBilling", "Churn"]
                              for column in binary_columns:
                              df[column] = df[column].map({"No": 0, "Yes": 1})</p>
                            </div>
                            Next column is 'MultipleLines'column has ['No phone service', 'No', 'Yes'] input. I think suffice to say we can group 'No phone service as No as well, so we map as [0,0,1] respectively. 
                            <div class="transparentbox">
                              <p>df['MultipleLines'] = df['MultipleLines'].map({"No phone service":0, "No":0, "Yes":1, })</p>
                            </div>
                            Moving on, is 'InternetService' columns which has ['DSL', 'Fiber optic', 'No'] input. From this, it is a bit akward to map it DSL or Fiber Optic as 0,1 where this input does not mean one is better than the other.
                            To circumvent this, we use the dummy function by create additional columns for each input, and automatically assign 0,1 for them.
                            Other columns 'Contract' and 'PaymentMethod', has the same unique input so we use the dummy function as well in one cell.
                            <div class="transparentbox">
                              <p>df = pd.get_dummies(data=df, columns=['InternetService'])<br>
                                df = pd.get_dummies(data=df, columns=['Contract'])<br>
                                df = pd.get_dummies(data=df, columns=['PaymentMethod'])</p>
                            </div>
                            The next column to check is the 'OnlineSecurity' columns. It has 'No' ,'Yes' ,'No internet service'. Similar to 'MultipleLines'column, we can map 'No Internet Service' as No as in 0. 
                            We can also group with 5 other colums that has the same input.  
                            <div class="transparentbox">
                              <p>binary2_columns = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']<br>

                                for column in binary2_columns:<br>
                                  df[column] = df[column].map({"No": 0, "Yes": 1, "No internet service":0})</p>
                            </div>
                            Lastly, the 'TotalCharges' column. From the input, it shows continous numbers as it should however it has being classify as object, probably wrongly formatted column. In order to rectify this,
                            we can use the pandas.to_numeric method, with parameters "coerce" any errors in conversion, the value become NaN.
                            <div class="transparentbox">
                              <p>df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors="coerce")</p>
                            </div>
                            Now all columns has being converted to integer/float, we do a simple check by using the sample function.
                            <div class="transparentbox">
                              <p>df.sample(5)</p>
                            </div>
                            <img src="img/sample2.png" alt="sam2" style="width:1000px"> 
                            <img src="img/sample3.png" alt="sam3"style="width:1000px"> 
                            <img src="img/sample4.png" alt="sam4" style="width:1000px"> 
                            We have a quick glance and see most columns has 0,1 input, and with addition a few columns as dummies as per our code before.
                            We also recheck if there is any NaN values after our conversion column 'TotalCharges'
                            <div class="transparentbox">
                              <p>df.info()</p>
                            </div>
                            <img src="img/info1.png" alt="info1"><br>
                            All columns has being converted into integer and float for easy analyzing, now to check any missing data in the table.
                            There is 11 missing input in the 'TotalCharges' column,  by comparing 7,043 sample from other columns that has non null vales with 7,032 samples in 'Total Charges' column.<br>
                            Since it is only 11 input from the total 7,043 count, i have decided to drop the missing row value from the column by using the function dropna since it is immaterial.
                            <div class="transparentbox">
                              <p>df = df.dropna()</p>
                            </div>
                            <u><br><b>Explore Data</b></u><br>
                            For exploring the data after the data scrubing, i would like to use the descriptive analysis function on the data, so let's try that.
                            <div class="transparentbox">
                              <p>df.describe()</p>
                            </div><br>
                            <img src="img/stats1.png" alt="stats1" style="width:1000px"> 
                            <img src="img/stats2.png" alt="stats2"style="width:1000px"> 
                            <img src="img/stats3.png" alt="stats3" style="width:1000px"><br>
                            We also can check that the all sample rows has being reduced to 7,032 input after dropna function<br>
                            General summary on the descriptive analysis based on the describe function shown<br>
                            1. The subscriber is divided around 50/50 by male & female<br>
                            2. the standard deviation churn is low with 0.441 which means the mean has +- deviation of 0.441782 from the mean. The lower is better.<br> 
                            3. The Mean of 26.57% churn rate means that out of 7032 samples, around a quarter of total sample churned from subscribing from the telco line.<br>
                            4. The average tenure for each customer out of 7032 samples is about 32 months (2 years and 8 months).<br><br>
                            Another way to explore data is using test significant variables.Testing significant variables often is done with correlation.<br>
                            The correlation can be find by using heatmap. By displaying a panda dataframe in Heatmap style, a visualization that is very useful 
                            in visualizing the concentration of values between two dimensions of a matrix. 
                            This helps in finding patterns and gives a perspective of depth.<br><br>
                            <u><b>Visualization Data as Heatmap</b></u><br>
                            Lets try the heatmap function as per below.
                            <div class="transparentbox">
                              <p>sns.set(rc={'figure.figsize':(20,20)})

                                corr = df.corr()
                                sns.heatmap(corr, annot=True,fmt=".2f")</p>
                            </div><br>
                            <img src="img/heatmap.png" alt="hm" style="width:1000px"><br>
                            From the heatmap, we can see a couple interesting correlation, the data churn and Contract_month_to month has a high correlation. 
                            This correlation could help us to find a batter model predictor for churn later on. <br>
                            While we also could point out that churn has a compratively negative relationship with tenure as well<br>
                            Lastly, we will utilise data visualisation to help us to identify significant patterns and trends in our data. We can gain a better picture through simple charts like 
                            line charts or bar charts to help us to understand the importance of the data.<br><br>
                            <u><b>Visualization Data as Countplot</b></u><br>
                            Here we use Countplot, visualization that is very useful to show the counts of observations in each categorical bin using bars. Since we are focusing on Churn data, 
                            I will use all other columns as the x axis in the countplot to find which column has best relatioship with the churn with the code as per below. 
                            <div class="transparentbox">
                              <p>main, subplots = plt.subplots(1, 4, figsize=(20,6))<br>
                                sns.countplot(data=df, x="gender", hue="Churn", ax=subplots[0])<br>
                                sns.countplot(data=df, x="SeniorCitizen", hue="Churn", ax=subplots[1])<br>
                                sns.countplot(data=df, x="Partner", hue="Churn", ax=subplots[2])<br>
                                sns.countplot(data=df, x="Dependents", hue="Churn", ax=subplots[3])</p>
                            </div><br>
                            <img src="img/chart1.png" alt="chart1" style="width:1000px"><br><br>
                            
                            <u>Findings from countplots</u><br>
                            1.   Gender Female, Male has around the same count to churn <br>
                            2.   Senior Citizens, Partner and Dependents has the same trend where numbers of churning is quite low compare to non churning.<br>
                            3.   Subcscriber who has Internet Service Fiber Optic to churn are are not that different that much.<br>
                            4.   PhoneService has different trend altogether, where phone service and non phone service are already varies so much. Subscribers
                             with phone service shows a large difference in not churning than churn<br><br>
                             <u><b>Visualization Data as Boxplot</b></u><br>
                             Another visualization method, create boxplot, a visualization for graphically depicting groups of numerical data through their quartiles. 
                             We wil use data x= Churn data, I will use other columns as the y axis in the boxplot as per below
                            <br>
                            <div class="transparentbox">
                              <p>main, subplots = plt.subplots(1,3, figsize=(16,10))<br>
                                sns.boxplot(data=df, x='Churn',y='tenure',ax=subplots[0])<br>
                                sns.boxplot(data=df, x='Churn',y='MonthlyCharges',ax=subplots[1])<br>
                                sns.boxplot(data=df, x='Churn',y='TotalCharges',ax=subplots[2])</p>
                              </div><br>
                              <img src="img/chart2.png" alt="chart" style="width:1000px"><br>
                              The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). 
                              The whiskers extend from the edges of box to show the range of the data.<br>
                              Findinds from boxplot<br>
                              1. You can find outliers in both tenure and Total Charges, they are data outside of Q3 as per the graph.<br>
                              2. Both tenure and Total Charges to churn has also low 1st quartile percentage till the median<br>
                              3. Monthly charges has a higher range in churn than the non churner.<br><br>
                            <u><b>Model Data</b></u><br>
                            In modelling the data, not all your features or values are essential to predicting your model. 
                            What you need to do is to select the relevant ones that contribute to the prediction of results. 
                            For accuracy, we need to use as much data as we can but relevant as well.
                            Here I will train models to forecasting future values.<br>
                            As we work with datasets, a machine learning algorithm works in two stages. We usually split the data around 20%-80% between testing and training stages.
                            We split a dataset into a training data and test data. For the columns i choose to include 'SeniorCitizen', 'Partner', 'PhoneService', 'DeviceProtection', 'OnlineSecurity', 'OnlineBackup', 'TechSupport', 
                            'PaperlessBilling', 'tenure', 'MonthlyCharges', 'TotalCharges'.<br>
                            Firstly, we group the columns involved as train_data, the features, data we used to predict the train, Churn as the train labels as it is what we wanted to train on.
                            <div class="transparentbox">
                              <p>columns = ['SeniorCitizen', 'Partner', 'PhoneService', 'DeviceProtection', 'OnlineSecurity', 
                                'OnlineBackup', 'TechSupport', 'PaperlessBilling', 'tenure', 'MonthlyCharges', 'TotalCharges' ]
                               train_data = df[columns]
                              train_labels = df['Churn']</p>
                            </div><br> <br>
                            <u><b>Importing Python Libraries</b></u><br>
                            As usual, we need to import the specific pyton libraries which is sklearn  train_test_split, decision tree and metrics as per below<br>
                            <div class="transparentbox">
                              <p>from sklearn.model_selection import train_test_split<br>
                                from sklearn import tree<br>
                                from sklearn import metrics</p>
                              </div>
                            Now we split arrays or matrices into random train and test subsets. The train size parametere is 30% proportion of the dataset 
                            to include in the train split. Another line is the decision tree model that we used to forecast decision, with parameter max_depth = 3, 
                            so it will at level 3 at max. And we called up the train data and train labels that was group before and use the function model.fit as 
                            Train Decision Tree Classifer
                            <div class="transparentbox">
                              <p>X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.3, random_state=1)<br>
                                model = tree.DecisionTreeClassifier(max_depth = 3)<br>
                                model.fit(train_data, train_labels)</p>
                              </div><br>
                              And now we used the sklearn predictions in predict the response for test dataset. In addition we will we use metrics.accuracy_score calculate the model accuracy.
                              <div class="transparentbox">
                                <p>y_pred = model.predict(X_test)<br>
                                  print ("Accuracy:", metrics.accuracy_score(y_test, y_pred))</p>
                                  </div>
                            <img src="img/accuracy.png" alt="acc"><br>     
                            Here after filtering and cleaning the data, we would get a 79.5% of accuracy if to model it in which i think that's a good indicator.
                            <br><br>    
                            <u><b>Visualizing Model Data as Decision Trees</b></u><br>
                            Additionally, You can visualize the Decision Tree alogorithm using the sklearn graphiz library.<br><br>
                            <u><b>Importing Python Libraries</b></u><br>
                            First we import libraries  sklearn graphviz function for display and plot the tree<br>
                            <div class="transparentbox">
                              <p>from sklearn.model_selection import train_test_split<br>
                                from sklearn import tree<br>
                                from sklearn import metrics</p>
                              </div> 
                              And we proceed with plotting  where we call up back the columns
                              <div class="transparentbox">
                                <p>fcolumns = list(train_data.columns)
                                  dot_data = tree.export_graphviz(model, out_file=None, feature_names=columns, class_names=["No", "Yes"], filled= True, rounded = True)
                                  
                                  graph = graphviz.Source(dot_data)
                                  graph</p>
                                </div>
                                <img src="img/decisiontree.png" alt="chart" style="width:1000px"><br>  
                                Findinds from decison tree<br>
                              1. Subscribers that has tenure that 16.5 months and monthly charges mroe than 68.25 are likely to churn with 35 out of the sample 7,032.<br>
                              2. The probability increases of not churning if their tenure with the telco more than 15.5 months.<br><br>
                              <u><b>Using The Model</b></u><br>  
                              Lets recap the model with the colums involved are 'SeniorCitizen' 'Partner', 'PhoneService', 'DeviceProtection', 'OnlineSecurity',
                               'OnlineBackup', 'TechSupport', 'PaperlessBilling', 'tenure', 'MonthlyCharges', 'TotalCharges'.<br> 
                               Let's take a scenario where subscriber has phone service, Device protection,online backup,tech support, 
                               paperlessbilling, 12 months contract, Monthly Charges of RM80 and Total Charges of RM 1000, what is the prediction of churning?<br>
                              "SeniorCitizen" :0, <br>
                              "Partner": 0,<br>
                              "PhoneService": 1,<br>
                              "DeviceProtection": 1,<br>
                              "OnlineSecurity": 0,<br>
                              "OnlineBackup": 1,<br>
                              "TechSupport":1,<br>
                              "PaperlessBilling": 1,<br>
                              "tenure": 12,<br>
                              "MonthlyCharges": 80,<br>
                              "TotalCharges":1000<br><br>
                              Lets put scenario input inside model.prediction and see how it goes
                              <p>model.predict([[0, 0, 1, 1, 0, 1, 1,1,12,80,1000]])</p> 
                              <img src="img/prediction1.png" alt="chart" style="width:500px"><br><br>
                              
                             <u><b>Interpreting Data</b></u> <br> 
                            The result 1 means customers with the scenario given would churn with accuracy model 79.5%. <br><br>
                            <u><b>Call To Actions/Suggestion to Telco Company To Retain Customers</u></b><br><br>
                            1. Increase clients with longer Tenure - To attract them by roll out better packages locking in long term contracts.<br>
                            2. Reduce Monthly Charges - To give some sort loyalty discounts or points to reduce monthly charges.<br>
                            3. Reduce Total Charges - To set competitive charges in line with the market, ike service charges, internet charges, overseas charges etc.
                            <br>
                            <u><b>Conclusion</u></b><br><br>

                            Other than relying on the model, we also need  to consider external factor like current situation like this COVID, market competitiveness of Telco in the country and technology.
                            One thing comes in mind is people are working from home & students are learning online, telcos need to readjust their packages to suit the current market need. 

                            <br><br><br>
                            </p>
                            </div>
                            </div></div></div>
                            <div class="tm-section-wrap bg-white">
                            <section id="portfolio2" class="row tm-section">
                            <div class="col-12 tm-section-pad">
                            <div class="tm-flex-item-left">
                            <div class="tm-w-80">
                            <h2 class="tm-color-primary tm-section-title mb-4">Word Clouds</h2>
                            <p class="mb-5"><img src="img/twitter.png" alt="telco" style="float:left;width:80px;height:80px;">
                            <p style="color: #069"; fontsize:10>Tweets</p> <br>
                            <h1 style="color:black; font-size:30px;">Making Word Clouds Based On Trump Tweets</h1><br>
                            The datasets and complete code can be found at the link below.<br>
                            Dataset link: <a href="https://www.kaggle.com/austinreese/trump-tweets?select=realdonaldtrump.csv"target="_blank">https://www.kaggle.com/austinreese/trump-tweets?select=realdonaldtrump.csv</a><br>
                            Project link: <a href="https://colab.research.google.com/drive/1ib-FHUnztA8MuVPSOVXnyGB-pha3j0kY?usp=sharing" target="_blank">https://colab.research.google.com/drive/1ib-FHUnztA8MuVPSOVXnyGB-pha3j0kY?usp=sharing</a><br><br>
                            The method I am using to analyze the dataset is by using Data Science OSEMN framework.<br>
                            In this portfolio, I am interested with words is he using the most in his tweets. How is he choosing these particular words to send his powerful messages, which effect instantly on stock market and the economic. 
                            Using visualization, creating a wordcloud with Donald Trump’s recent tweets to explore his most used words.
                            <br><br>
                            <u><b>Importing Python Libraries</b></u><br>
                            First, we need to import the necessary python libraries for this analysis. These include the basic packages such as
                            pandas, numpy, os import path, PIL import inage, wordcloud as well stopwords,imagecolor generator and urllib and lastly matplotlib pyplot and inline.
                            
                            <div class="transparentbox">
                            <p>import numpy as np<br>
                              import pandas as pd<br>
                              from os import path<br>
                              from PIL import Image<br>
                              from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator<br>
                              import urllib<br>
                              import matplotlib.pyplot as plt<br>
                              % matplotlib inline</p>
                            </div><br><br>
                            <u><b>Data Source</b></u><br>
                            The data that I am using for this portfolio is the raw dataset in kaggle as per link prior. It is stated that the tweets are from July 2020 to October 2020
                            In a quick glance it contains columns and rows with mostly words & numbers in the table.
                            I will use Python to perform all data analysis and visualization. This data is originally in a CSV file and we will 
                            convert it into a data frame by using Pandas and let’s take a look at what we have.<br>
                            <div class="transparentbox">
                            <p>df = pd.read_csv("https://raw.githubusercontent.com/kudin36/DataScience<br>/main/realdonaldtrump.csv")<br>
                            df</p>
                            </div>
                            <img src="img/tweet1.png" alt="tweet1" style="width:1000px"> 
                            Our focus is the ‘content’ column which contains the words of Trump tweet. Mind on the NaN values as well.<br><br>
                            <u><b>Data Scrubbing</b></u><br>
                            Next, we will check if the columns have any null values.
                            
                            <div class="transparentbox">
                            <p>df.isnull().sum()</p>
                            </div>
                            <img src="img/tweet2.png" alt="tweet2"><br>
                            There are NaN values in 'mentions'and 'hashtags'column but our focus is the ‘content’ column which contains the tweets. We can drop the other columns but I will just keep them in the data 
                            frame since they won’t impact my wordcloud.<br>
                            <br><br>
                            Next we look data type of each value in the columns focusing on the 'content' column.
                            <div class="transparentbox">
                              <p>df.dtypes</p>
                              </div>
                              <img src="img/tweet3.png" alt="tweet3"><br>

                              <div class="transparentbox">
                                <p>stopwords = set(STOPWORDS)</p>
                                </div>
                            <img src="img/tweet3.png" alt="tweet3"><br><br>
                            <b>Visualizing Data as Wordcloud</b><br>
                            One of the function that we want use as one of the parameter is stopwords where it will ignore commonly used words such “the”, “a”, “an”, “in”
                            Set it and use another line code with the plotting pyplot,Imagegenerator, and other parameters like width and background color
                            <div class="transparentbox">
                              <p>stopwords = set(STOPWORDS)</p>
                            </div>
                            <div class="transparentbox">
                            <p>def wordcloud_generator(data, title=None):<br>
                              wordcloud = WordCloud(width = 1000, height = 1000,<br>
                                                    background_color ='white',<br>
                                                    stopwords=stopwords,<br>
                                                    min_font_size = 10<br>
                                                   ).generate(" ".join(data.values))<br><br>
                              
                              # plot the WordCloud image<br>                        
                              plt.figure(figsize = (16, 20), facecolor = None)<br> 
                              plt.imshow(wordcloud, interpolation='bilinear') <br>
                              plt.axis("off") <br>
                              plt.tight_layout(pad = 0)<br> 
                              plt.title(title,fontsize=30)<br>
                              plt.show() </p>
                              </div> <br>
                              
                              <img src="img/tweet3.png" alt="tweet3"><br>
                              Now we use wordcloud function and put a title on the image "Most used words in Trump Tweets"
                              <div class="transparentbox">
                              <p>wordcloud_generator(df['content'], title="Most used words in Trump Tweets")</p></div><br>
                              </div>
                              <img src="img/wordcloud1.png" alt="wc1"style="width:800px"><br>
                              Here,  we could see trump words in different sizes, is organized as per picture above. For a better visualization, 
                              I want to use a mask image Trump and the words are organized accordingly to the mask body.
                              Mask Images can be foound in google image, preferably a PNG as it will be nicer. 
                              Using the imported libraries urlib and numpy, we can callup the image mask as per line below
                              <div class="transparentbox">
                              <p>trump_mask = np.array(Image.open(urllib.request.urlopen('https://raw.githubusercontent.com/kudin36<br>/DataScience/main/masktrump.png')))</p></div>
                              And then for our reference we plot the mask image according to our needed width and sizes as the line below using plt function to check it.
                              <div class="transparentbox">
                              <p>fig = plt.figure(figsize=(16, 20))<br>
                              <br>
                              plt.imshow(trump_mask, cmap=plt.cm.gray, interpolation='bilinear')<br>
                              plt.axis('off')<br>
                              plt.show()</p></div>
                              <img src="img/masktrump.png" alt="tweet4"style="width:500px"><br>
                              Looks like the image mask is ok, here we proceed to called again the line to visualize the word cloud but with a new parameter trump_mask<br>
                              <div class="transparentbox">
                                <p>def wordcloud_generator(data, title=None):<br>
                                  wordcloud = WordCloud(width = 1000, height = 1000,<br>
                                                        background_color ='white',<br>
                                                        mask=trump_mask,<br>
                                                        stopwords=stopwords,<br>
                                                        min_font_size = 10<br>
                                                       ).generate(" ".join(data.values))<br><br>
                                  
                                  # plot the WordCloud image<br>                        
                                  plt.figure(figsize = (16, 20), facecolor = None)<br> 
                                  plt.imshow(wordcloud, interpolation='bilinear') <br>
                                  plt.axis("off") <br>
                                  plt.tight_layout(pad = 0)<br> 
                                  plt.title(title,fontsize=30)<br>
                                  plt.show() </p>
                                  </div>
                              <div class="transparentbox">
                              <p>wordcloud_generator(df['content'], title="Most used words in Trump Tweets")</p>
                              </div><br>
                              <img src="img/wordcloud2.png" alt="tweet5"style="width:1000px"><br>
                              <br>
                              Alright, now we have word cloud of Trump Tweet in his own Trump mask image with his iconic and unique hairstyle.<br><br>
                              <u><b>Interpreting Data</b></u><br>
                              Whoever been keeping tabs on Donald’s twitter, will probably be familiarised with his tweets. Most tweeted words contain “Biden”, “America Great” and “Fake News”. 
                              It is also interesting that even after he won presidential election he still keep mentioning “BarackObama” which is the most tweeted words.<br>
                              <b>Conclusion</b><br>
                              Wordclouds are useful for data exploration and analysis in NLP projects.
                              They are great to visualize words in a creative way. This visualization can add values to other projects as well.
                
                                </div> 
                            </div></div></div>          
                            <div class="tm-section-wrap bg-white">
                            <section id="portfolio3" class="row tm-section">
                            <div class="col-12 tm-section-pad">
                            <div class="tm-flex-item-left">
                            <div class="tm-w-80">
                            <h2 class="tm-color-primary tm-section-title mb-4">Machine Learning & Linear Regression</h2>
                            <p class="mb-5"><img src="img/socialmedia.png" alt="sm" style="float:left;width:80px;height:80px;">
                            <p style="color: #069"; fontsize:10>Social Media</p> <br>
                            <h1 style="color:black; font-size:30px;">Forecasting Budget For Social Media Ads</h1><br>
                            The datasets and complete code can be found at the link below.<br>
                            Dataset link: <a href="https://github.com/theleadio/datascience_demo/blob/master/social-ads-raw.xlsx?raw=true"target="_blank">https://github.com/theleadio/datascience_demo/blob/master/social-ads-raw.xlsx?raw=true</a><br>
                            Project link: <a href="https://colab.research.google.com/drive/1FwrvrVJe55ZeUilmI-JSnqT0BwHY84OC?usp=sharing" target="_blank">https://colab.research.google.com/drive/1FwrvrVJe55ZeUilmI-JSnqT0BwHY84OC?usp=sharing</a><br><br>
                            
                            I will be using the Data Science OSEMN framework to analyze the data. Here I want to show how to use linear regression for modelling.
                            <br><br>
                            <u><b>Importing Python Libraries</b></u><br>
                            First, we need to import the necessary python libraries for this analysis. These include the basic packages such 
                            pandas, seaborn and matplotlib. In addition pyplot and sklearn linear model
                            
                            <div class="transparentbox">
                              <p>import pandas as pd
                                import seaborn as sns
                                import matplotlib.pyplot as plt
                                from sklearn.linear_model import LinearRegression</p>
                            </div><br><br>
                            <u><b>Data Source</b></u><br>
                            The data that I am using for this portfolio is the raw dataset sample given in the class as per link dataset before. 
                            From the look if it, contains columns and rows with numbers and 2 columns with numbers in the table.
                            I will use Python to perform all data analysis and visualization. This data is originally in a CSV file and we will 
                            convert it into a data frame by using Pandas.<br>
                            <div class="transparentbox">
                             <p>df = pd.read_excel("https://github.com/theleadio/datascience_demo/<br>
                              blob/master/social-ads-raw.xlsx?raw=true")</p></div>
              
                            Lets use the function df to see what the data is all about
                            <div class="transparentbox">
                              <p>df</p>
                            </div>
                            <img src="img/socio1.png" alt="socio1" style="width:500px"><br>
                            Our focus is the will be social ads column facebook, google & instagram column<br><br>
                            <u><b>Data Scrubbing</b></u><br>
                            Next, we will check if the columns have any null values.
                            <div class="transparentbox">
                              <p>df.isnull().sum()</p>
                            </div>
                            <img src="img/socio2.png" alt="socio2"><br>
                            Look like no NaN values. So we can proceed with other cleaning.<br> 
                            Next we look data type of each value in the columns
                            <div class="transparentbox">
                              <p>df.info()</p>
                            </div>
                            <img src="img/socio3.png" alt="socio3"><br>
                            Looking at each 7 column value data are integer and float input except for 'size' & 'area' columns
                            We try to convert the 2 columns into numerical. We group the columns as col for easier call up and use the function unique to check.<br>
                            <div class="transparentbox">
                            <p>col = ['size', 'area']<br>

                            for column in col:<br>
                            print(column, ':', df[column].unique(), '\n')</p>
                            </div>
                            Let see the result.<br>
                            <img src="img/socio4.png" alt="socio4"><br>
                            'size' column has 2 input  while 'area' column has 3.
                            First column 'size'', we can convert large, small data object to numerical 1 & 0 respectively using mappping function as per below.
                            <div class="transparentbox">
                              <p>df['size'] = df['size'].map({"large":1, "small":0})<br>
                              df</p>
                            </div>
                            <img src="img/socio5.png" alt="socio5"><br>
                            However upon checking, after using the map function, the column size has NaN values due to the data is an immutable type and cannot be converted.
                            To circumvent this we need to create a new column to map the the data large, small into 1 & 0. In addition we delete the coding previously
                            because that's  not what we want.
                            <div class="transparentbox">
                              <p>df['is_large'] = df['size'].map({"small":0, "large":1})<br>
                              df</p>
                            </div>
                            <img src="img/socio6.png" alt="socio6"><br>
                            Next one is 'area' columns which have ['rural' 'urban' 'suburban']. Since the input has unique rural,urban & suburban we cannot change it to 0,1 
                            since the input does not mean one input is better than the other. To solve this need to create dummy variables for these columns<br>
                            Another way of creating dummy is create another group set of dummy columns and mantaining the original column as it is ,
                            however we need to join that new group columns to df to make it work.
                            <div class="transparentbox">
                              <p>df = df.join(area_dummies)
                              </p>
                            </div>
                            <div class="transparentbox">
                              <p>df = df.join(area_dummies)
                              </p>
                            </div>
                            Let see how our table look like after the mapping and dummy.
                            <img src="img/socio7.png" alt="socio7"><br>
                            Looks like a good table for analyzing.<br><br>

                            <u><b>Explore Data</b></u><br>
                            For exploring the data after the data scrubing, I would like to use scatterplot to visualize the data.
                            It is a visualization technique that shows the relationship between two numerical variables. As we can see the three column google, facebook & instagram
                            has continous number as its input, thus can be use in a scatter plot.<br><br>
                            <u><b>Visualizing Data as Scatterplot</b></u><br>
                            We plot 3 scatterplot, column google, facebook & instagram side by side. We first called up the body of the plot as per line below using the plt library to check.
                            <div class="transparentbox">
                              <p>
                                fig, subs = plt.subplots (1,3)</p>
                            </div>
                            <img src="img/socio8.png" alt="socio8"><br>
                            Look like a bit cramp where labels overlap. Let's add figsize parameter to widen the plot size.
                            <div class="transparentbox">
                              <p>
                                fig, subs = plt.subplots (1,3, figsize=(15,8))</p>
                            </div><br>
                            <img src="img/socio9.png" alt="socio9" style=width:800px><br>
                            Looks better, so set scatter plot with the size that we check before using the plot function regplot, set column google, facebook and instagram as x axis, 
                            'sales'column as our y axis to see how each of their relationship.
                            <div class="transparentbox">
                              <p>fig, subs = plt.subplots (1,3, figsize=(15,8))<br><br>

                                sns.regplot(data=df, x="google", y="sales", ax=subs[0])<br>
                                sns.regplot(data=df, x="facebook", y="sales", ax=subs[1])<br>
                                sns.regplot(data=df, x="instagram", y="sales", ax=subs[2])<br>
                               </p>
                            </div><br>
                            <img src="img/socio10.png" alt="socio10" style="width:800px"><br><br>
                            Findinds from scatterplot<br>
                            1.   All three social ads has positive relationship with sales. <br>
                            2.   However google a lot more point sets near to the line,facebook line is similar to google, but more disperse, 
                            while instagram is more scatter than all of them. <br>
                            3.   In addition, instagram has more weak points where some points are futher than the rest of them which show a 
                            really low positive relationship.<br>
                            Now after the scrubing and a bit of exploration of the data, we can moving on to modelling data.<br><br>
                            <u><b>Model Data</b></u><br>
                            In modelling the data, not all your features or values are essential to predicting your model. 
                            What you need to do is to select the relevant ones that contribute to the prediction of results. 
                            For this portfolio, i will be using Linear Regression function to forecast future values<br>
                            To explain linear regression can be taxing & difficult for a non maths background like me. But to summarize in a simple,
                             we can relate to the mathematical function y=ax+b that we learn in schools.<br>
                            <img src="img/socio11.png" alt="socio11" style="width:500px"><br>
                            Linear regression follows the assumption that with all the data you’ve found so far, you can draw a line [y = ax + b] to identify 
                            a trend with these points and then figure out what y is at any value that x is.<br>
                            For the workings & formulas behind it, you can read further on the link <a href="https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a"target="_blank">here.</a><br>
                            Anyway we set column social ads as our x function, y to sales. Here we want how much revenue driven by the budget. Let's try first google as per line below.
                            <div class="transparentbox">
                              <p>features = ['google']<br>
                                x = df[features]<br>
                                y = df['sales']</p>
                            </div>
                            <div class="transparentbox">
                              <p>model = LinearRegression()
                                model.fit(x, y)</p>
                            </div><br> 
                            Now we have a linear regression, we can check their R-Square, Intercept & Coefficient as per line below.
                            <div class="transparentbox">
                              <p>print("R Square", model.score(x,y))<br>
                                print("Intercept",  model.intercept_)<br> 
                                print("Coefficient",  model.coef_)</p>
                            </div><br>  
                            <img src="img/socio12.png" alt="socio12" style="width:500px"><br>
                            In general, R Square is basically the evaluing the performance of the model by hows how well the data fit the regression model, 
                            Intercept as in the b in y=ax+b function is the constant when x=0 , while coefficientis the a in y=ax+b, the costant that which the variables multiplied.<br>
                            Now we have input in the equation,let do some forecasting.<br>
                            Just say we allocate 100 units in the google social ads.
                            <div class="transparentbox">
                            <p>#y = ax + b<br>
                              #let allocate google budget 100<br> 
                              (0.04576465*100)+7.032593549127695</p>
                            </div><br>
                            <img src="img/socio13.png" alt="socio13"><br>
                            Now we have have model output, the y in y=ax+b.<br>
                            Let also do a forecasted revenue based on the y that we get. We assume unit price of the unit sold as RM 2,500.
                            <div class="transparentbox">
                              <p>unit_price = 2,500<br>
                                expected_revenue = 11.609058549127695*1000*unit_price
                              <br>
                              print(F"Expected Revenue: {expected_revenue:,.2f}")
                              </p>
                            </div><br>
                            And we get
                            <img src="img/socio14.png" alt="socio14"><br>
                            Now we can advanced model on incorporating the other 2 social ads together. Lets do the coding before with adding facebook & instagram column
                            in features and run the linear regression again.
                            <br>
                            <div class="transparentbox">
                              <p>features = ['google', 'facebook', 'instagram']<br>
                             x = df[features]<br>
                             y = df['sales']
                              </p>
                            </div>
                            <div class="transparentbox">
                              <p>model = LinearRegression()<br>
                                model.fit(x, y)
                              </p>
                            </div>
                            Next we find the R2, Intercept and Coefficient.
                            <div class="transparentbox">
                              <p>print("R Square", model.score(x,y)) 
                                print("Intercept",  model.intercept_)<br>
                                print("Coefficient",  model.coef_)
                              </p>
                            </div>
                            <img src="img/socio15.png" alt="socio15"><br><br>
                            
                            <u>Findings from Model</u><br>
                            This Rsquare is higher than first one, meaning more variables make the model more accurate<br>
                            The Intercept is lower, the constant of the equation reduce significantly.
                            From the coeffticient, facebook has a higher multiplier than google, instagram has negative coefficient, which reduceds the value if used<br>
                            <br>
                            
                            <br>
                            
                            <img src="img/socio14.png" alt="socio14"><br>
                            Let do a further and better model. Lets incorporate the other is_large and the dummy columns into our model.<br>
                            We redo the linear regression coding as well adding with new features and check the R2, Intercept and Coefficient.
                            <div class="transparentbox">
                              <p>features = ['google', 'facebook', 'instagram', 'is_large', 'rural', 'suburban', 'urban']<br>
                                x = df[features]<br>
                                y = df['sales']
                              </p>
                            </div>
                            <div class="transparentbox">
                              <p>model = LinearRegression()<br>
                                model.fit(x, y)
                              </p>
                            </div>
                            <div class="transparentbox">
                              <p>print("R Square", model.score(x,y)) 
                                print("Intercept",  model.intercept_)<br>
                                print("Coefficient",  model.coef_)
                              </p>
                            </div>
                            <img src="img/socio16.png" alt="socio16"><br><br>
                          <u>Findings from Model</u><br>

                            1. This Rsquare is increase a bit, still a better model to use<br>
                            2. The Intercept is lower a bit,it shows minimal changes in the multiplier<br>
                            3. From the coeffticient, is_large and urban column has positive relationship, while rural and suburban has negative relationship.


                            Based on the this, we can find rank the social ads variables importance when we do model.<br>
                            1. Facebook<br>
                            2. Google<br>
                            3. urban<br>
                            4. is_large<br>
                            5. rural<br>
                            6. sub_urban<br>
                            7. instagram<br><br>

                            Let take a scenario, where we have 100 units of social ads to have, allocate to which social ads and segment to maximize the revenue.<br>
                            Maximize profit method - allocate all in facebook since it has the largest positive coefficient, and also choose urban & is_large as well.
                            <div class="transparentbox">
                              <p>model.predict([[0, 100, 0, 1, 0, 0, 1]])</p>
                              </div><br>
                              <img src="img/socio17.png" alt="socio17"><br>
                              <div class="transparentbox">
                                <p>unit_price = 2500<br>
                                  expected_revenue2 = 22.00639517 * 1000 * unit_price<br>
                                  print(F"Expected Revenue: {expected_revenue2:,.2f}")</p><br></div>
                                  <img src="img/socio18.png" alt="socio18"><br>

                            Based on this model, we know the expected revenue by maximizing facebook we will get RM 55,015,987.92, since there is no limit which to prioritize. <br><br>
                            <u>Findings from Model</u><br>
                            1. More variables are better for better model, proven by the increasing R Square value each time column are added.<br>
                            2. In the model, we need to find columns that has positive coefficient, to rank which is better for the forecasting.<br><br>

                            <u><b>Interpreting Data</b></u><br>
                            Sugesstion on Social Ads Budget.<br>
                            1. Focus on Facebook and Google social ads for more revenue.<br>
                            2. Focus on large segment and urban locality to increase sales .<br>
                            Alright, i find that linear regression is quite a fun model to play around. In addition, I believe you don't need to memorize the complexity of Linear Regression, 
                            but learn how to apply for it. So don't be afraid of the technicality behind it. 
                                </div>
                            </p>
                            </div>
                            </div></div></div>
                            <div class="tm-section-wrap bg-white">
                            <section id="portfolio4" class="row tm-section">
                            <div class="col-12 tm-section-pad">
                            <div class="tm-flex-item-left">
                            <div class="tm-w-80">
                            <h2 class="tm-color-primary tm-section-title mb-4">Sentiment Analysis</h2>
                            <p class="mb-5"><img src="img/tripadvisor.png" alt="ta" style="float:left;width:80px;height:80px;">
                            <p style="color: #069"; fontsize:10>TripAdvisor</p> <br>
                            <h1 style="color:black; font-size:30px;">Sentiment Analysis On Tripadvisor Reviews</h1><br>
                            The datasets and complete code can be found at the link below.<br>
                            Dataset link: <a href="https://raw.githubusercontent.com/theleadio/datascience_demo/master/tripadvisor_hotel_reviews.csv" target="_blank">https://raw.githubusercontent.com/master/tripadvisor_hotel_reviews.csv</a><br>
                            Project link: <a href="https://colab.research.google.com/drive/1rVvU4m0lTDYt8YQTcAjZjBCxZWvXAcXm?usp=sharing" target="_blank">https://colab.research.google.com/drive/1rVvU4m0lTDYt8YQTcAjZjBCxZWvXAcXm?usp=sharing</a><br>
                            <br>
                            The method I am using to analyze the dataset is by using Data Science OSEMN framework.<br>
                            For this project, I am interested with evaluating ratings/reviews in TripAdvisor website by using Sentiment Analysis.
                            <br><br>
                            <u><b>Importing Python Libraries</b></u><br>
                            First, we need to import the necessary python libraries for this analysis. These include the basic packages such as
                            pandas, seaborn, matplotlib. We will be importing as well wordcloud, nltk and specifically the VADER Sentiment Analysis 
                            (Valence Aware Dictionary and Sentiment Reasoner)
                            
                            <div class="transparentbox">
                            <p>import pandas as pd<br>
                              import seaborn as sns<br>
                              import matplotlib.pyplot as plt<br>
                              from wordcloud import WordCloud<br>
                              import nltk<br>
                              from nltk.sentiment.vader import SentimentIntensityAnalyzer<br>
                              nltk.download('vader_lexicon')</p>
                            </div><br>
                            <u><b>Data Source</b></u><br>
                            The data that I am using for this portfolio is the raw dataset in github , sample from class as per link prior.
                            Looking at the table, it has 2 columns reviews and ratings  which in words and numbers respectively.
                            I will use Python to perform all data analysis and visualization. This data is originally in a CSV file and we will 
                            convert it into a data frame by using Pandas and let’s take a look at what we have.<br>
                            <div class="transparentbox">
                            <p>df = pd.read_csv('https://raw.githubusercontent.com/theleadio/<br>datascience_demo/master/tripadvisor_hotel_reviews.csv')<br>
                            </p>
                            </div>
                            <div class="transparentbox">
                              <p>df.sample(5)<br>
                              </p>
                              </div>
                            <img src="img/trip1.png" alt="trip1" style="width:1000px"> 
                            Both columns are important for our analysis<br><br>
                            <u><b>Data Scrubbing</b></u><br>
                            Next, we will check if the columns have any null values.
                            
                            <div class="transparentbox">
                            <p>df.isnull().sum()</p>
                            </div>
                            <img src="img/trip2.png" alt="trip2"><br>
                            Both columns do not have null values.
                            Next we look data type of each value in the columns.
                            <div class="transparentbox">
                              <p>df.info()</p>
                              </div>
                              <img src="img/trip3.png" alt="trip3">
                            </p>
                            </div>
                            So far there is no scrubing need to be done because there is no null values, 
                            and their data type is what we expect to do our sentiment analysis.<br><br>
                            <u><b>Explore Data</b></u><br>
                            Now for exploring the data after the data scrubing, I would like to use countplot to visualize the data.
                            It is a visualization technique that very useful to show the counts of observations in each categorical 
                            bin using bars. Since we are focusing on Rating data, I will use ratings columns as the x axis in the countplot.<br><br> 
                            <u><b>Visualization Data as Countplot</b></u><br>
                            <div class="transparentbox">
                              <p>sns.set(rc={'figure.figsize':(10,8)})<br>
                                sns.countplot(data=df, x='Rating', color='blue')</p>
                              </div>
                              <img src="img/trip4.png" alt="trip4">
                            </p>
                            </div><br><br>
                            <u>Findings from countplot</u><br>
                            1. From the countplot, we can see a couple interesting counts. Most reviews has rating 4 and 5, which show favorable view on services
                            used through tripadvisor.<br>
                            2. While we also could point out that rating 1,2 and 3 has around the same count, but if we totalled it up, 
                            total rating 1 - 3 has roughly the same as rating 4. It shows that unfavourable/mediocre ratings count for 30% of the review,
                            which could be alarming to tripadvisor.<br><br>

                            <u><b>Visualization Data as Word Cloud</b></u><br>
                            Create Word Cloud, a visualization technique for texts that are natively used for visualizing the tags or keywords from the websites. 
                            These keywords typically are single words that depict the context of the webpage the word cloud is being made from. These words are 
                            clustered together to form a Word Cloud. Lets use the word cloud coding and set the plot as per coding below.
                            <div class="transparentbox">
                              <p>def wordcloud_generator(data, title=None):<br>
                                wordcloud = WordCloud(width = 800, height = 800,<br>
                                                      background_color ='black',<br>
                                                      min_font_size = 10<br>
                                                     ).generate(" ".join(data.values))<br><br>
                                                      
                                plt.figure(figsize = (8, 8), facecolor = None)<br> 
                                plt.imshow(wordcloud, interpolation='bilinear')<br> 
                                plt.axis("off")<br> 
                                plt.tight_layout(pad = 0)<br> 
                                plt.title(title,fontsize=30)<br>
                                plt.show() </p>
                              </div><br>
                              Let see the overall words in the review column.<br>
                              <div class="transparentbox">
                                <p>wordcloud_generator(df['Review'], title="Most used words in reviews")</p>
                                </div><br>
                              <img src="img/trip5.png" alt="trip5"><br><br>
                              <u>Findings from Word Cloud</u><br>
                              From the word cloud, we can several words are being visualized as more used such as 'hotel', 'room'<br>
                              'resort' and others. This word cloud doesn't tell much on how the words are being used to in its own. 
                              <br>Let set word cloud reviews based on ratings 1 to ratings 5.<br>
                              <div class="transparentbox">
                                <p>or rating in range(1, 6):  #rating 1 to 5, 6 is not included as integer
                                  reviews = df[df['Rating'] == rating]
                                  wordcloud_generator(reviews['Review'], title="Most used words for rating " + str(rating))
                                </p>
                                </div><br>
                                <img src="img/trip6.png" alt="trip6" style="float:left;width:400px;height:400px;">
                                <img src="img/trip7.png" alt="trip7" style="float:left;width:400px;height:400px;">
                                <img src="img/trip8.png" alt="trip8" style="float:left;width:400px;height:400px;">
                                <img src="img/trip9.png" alt="trip9" style="float:left;width:400px;height:400px;">
                                <img src="img/trip10.png" alt="trip10" style="width:400px;height:400px;"><br><br>
                                <u>Findings from Word Cloud - Rating 1 to Rating 5</u><br><br>

                                1. Mostly in general, the bigger words contain hotel, restaurant, good, resort, room.<br>
                                2. Rating 4 and 5 have around the same words visualized.<br>
                                3. Rating 1,2 & 3 you can see some negative words being visualized, although is not 
                                visualized as much as words in point no.1. Probably each reviewers has unique negative 
                                reviews based on their expriences.<br>
                              <b>Model Data</b><br>
                              For words, we can use IDF-Vectorizer for Machine Learning to help us in modeling. Afterward we can use Sentiment Analyzer
                              to analyze further on the words.<br><br>
                              <u>Modelling Using Query TF-IDF Vectorizer</u><br>
                              Next, we want to create query using a vectorizer.<br>
                              It is a statistical measure that evaluates how relevant a word is to a document in 
                              a collection of documents. This is done by multiplying two metrics:<br>
                              how many times a word appears in a document
                              the inverse document frequency of the word across a set of documents. <br>
                              Again, for a non mathematics background for me, explaning the function and its working it's a bit complex for me
                              but in a nutshell, it's transforming words into numerical value or vector.<br>
                              You can read more on the workings and formulas associated from this link <a href="https://monkeylearn.com/blog/what-is-tf-idf/"target="_blank">here</a>
                              Lets do a query on the top 5 reviews using the words 'Hotel'. Firsty we called up the function Vectorizer from the nltk library.<br>
                              We set stop words English to filer out words which does not add much meaning to a sentence. And we add in the column Review.
                              <div class="transparentbox">
                                <p>tfidf = TfidfVectorizer(stop_words='english')<br>
                                  feature = tfidf.fit_transform(df['Review'])
                                </p>
                                </div>
                              <div class="transparentbox">
                                <p>query = "hotel"
                                  queryTFIDF = tfidf.transform([query])
                                  cosims = cosine_similarity(queryTFIDF, feature).flatten()
                                  results = cosims.argsort()[:-6:-1]#top 5
                                  
                                  for r in results:
                                    print(df.iloc[r]['Review'])
                                    print("----")
                                </p>
                                </div><br>
                                And we get the input as per below. 
                                <img src="img/trip13.png" alt="trip13" style="height:300"><br><br>
                                <u>Findings from query TF-IDF Vectorizer</u><br><br>

                                1. We can see top 5 review with the words on hotel. We can see the word hotels are 
                                used couple of times in the same sentences and altogether in the review. <br>
                                In addition here we can use other words as well if you wish.<br><br>

                                <u>Modelling using Sentiment Analysis</u><br>
                                This sentiment analysis help you determine the ratio of positive to negative engagements about a specific topic using the capabilities of 
                                the Natural Language Toolkit (NLTK). Here we would like to the sentiments of the words used but relating to the rating.
                                <div class="transparentbox">
                                <p>tfidf = TfidfVectorizer(stop_words='english')<br>
                                  feature = tfidf.fit_transform(df['Review'])
                                </p>
                                </div>
                                <div class="transparentbox">
                                <p>df['Sentiments'] = sentiments
                                </p>
                                </div>
                                Let check the df
                                <div class="transparentbox">
                                  <p>df.head()
                                  </p>
                                  </div>
                                  <img src="img/trip14.png" alt="trip14"style="width:1000px;">
                                  Here we can see the review, rating and a new column sentiments. In column sentiment, positive number show good 
                                  sentiment while negative number. Let set a new code, to find rating 5 but with ranking with the top 5 negative sentiments.
                                  <div class="transparentbox">
                                    <p>df[(df['Rating'] == 5) & (df['Sentiments'] <0)].sort_values("Sentiments")[0:5]
                                    </p>
                                    </div>
                                    <img src="img/trip15.png" alt="trip15" style="width:1000px"><br>
                                  Ok, Let do reversal of it, to find rating 1 but with the ranking of top 5 positive sentiments.
                                  <div class="transparentbox">
                                    <p>df[(df['Rating'] == 1) & (df['Sentiments'] >0)].sort_values("Sentiments")[0:5]
                                    </p>
                                    </div>
                                    <img src="img/trip16.png" alt="trip16" style="width:1000px"><br>
                                    We can also look by the top 5 lowest sentiment with the code below.
                                    <div class="transparentbox">
                                      <p><df[(df['Rating'] == 1) & (df['Sentiments'] >0)].sort_values("Sentiments", ascending = False)[0:5]
                                      </p>
                                      </div>
                                      <img src="img/trip17.png" alt="trip17" style="width:1000px"><br><br>
                                      <u><b>Interpreting Data</b></u><br>
                                      Findings from sentiments.<br>
                                      1. People tend to write longer in complain/negative reviews.<br>
                                      2. The unfavourable sentiments are quite specific.<br>
                                      3. Favourable ratings are mostly generic words and short.<br>
                                      4. Favourable ratings does not mean overall a good sentiments and vice versa.<br>

                                      Sugesstion to Tripadvisor.<br>
                                      1. Should introduce a better rating system for the products/services. <br>
                                      2. Reviews can be more focus oriented, with selection on which area are to be review on.<br>
                            
                            
                            
                            
                            
                            
                            
                            <br><br><br>
                            </div></div></div>
                            <div class="tm-section-wrap bg-white">
                            <section id="portfolio5" class="row tm-section">
                            <div class="col-12 tm-section-pad">
                            <div class="tm-flex-item-left">
                            <div class="tm-w-80">
                            <h2 class="tm-color-primary tm-section-title mb-4">Dashboard</h2>
                            <p class="mb-5"><img src="img/HR.png" alt="hr" style="float:left;width:80px;height:80px;">
                            <p style="color: #069"; fontsize:10>HR</p> <br>
                            <h1 style="color:black; font-size:30px;">Create Online Dashboard Google Data Studio To Analyse HR Atrition</h1><br>
                            The datasets and complete code can be found at the link below.<br>
                            Dataset link: <a href="https://raw.githubusercontent.com/theleadio/datascience_demo/master/HR_dataset.csv"target="_blank">https://raw.githubusercontent.com/theleadio/datascience_demo/master/HR_dataset.csv</a><br>
                            Project link: <a href="https://datastudio.google.com/reporting/98329e03-a8ff-47a4-8e62-ddc149912d47" target="_blank">https://datastudio.google.com/reporting/98329e03-a8ff-47a4-8e62-ddc149912d47</a><br>
                            <br>
                            <iframe width="900" height="700" src="https://datastudio.google.com/embed/reporting/98329e03-a8ff-47a4-8e62-ddc149912d47/page/rUSXC" frameborder="0" style="border:0" allowfullscreen></iframe>
                            <br>
                            Here, I want to create an interactive online dashboard using Google Analytics which shown above. I am looking for Staff attrition specifically, which refers to the loss of employees through a natural process, 
                            such as retirement, resignation, elimination of a position, personal health, or other similar reasons.
                            <br>Dashboard is a good way to give data visualization tool that tracks, analyzes, and displays KPIs, metrics, and critical data points. 
                            Dashboards empower both technical and non-technical users to understand and leverage business intelligence to make more informed decisions. <br><br>
                            <u><b>Data Source</b></u><br>
                            The data that I am using for this portfolio is the raw dataset sample given in the class as per link dataset earlier on. It is a dataset that contains HR related input.<br>
                            Lets review the data first before we use Google Data Studio.<br>
                            Overview<br>

                            names - staff name<br>
                            satisfaction_level - level of satisfaction of staffs working<br>
                            last_evaluation - latest performance rating.<br>
                            number_project - number of project handlle by each staff<br>
                            average_monthly_hours - Average working hours of each staff<br>
                            exp_in_company - Individual years in the company<br>
                            work_accident	- whether staff had work accident, is shown as integer, 0 is No, 1 is Yes<br>
                            left - whether staff left the company, is shown as integer, 0 is No, 1 is Yes<br>
                            promotion_last_5years	 - this is whether staff has received promotion on the last 5 years, is shown as integer, 0 is No, 1 is Yes<br>
                            role - Position of each staff according to sales, technical,support, IT,product_mng, marketing, RandD, accounting, hr, management<br>
                            salary - this is the salary range of the staff according to low, medium and high.<br><br>
                            Looks like good table for us to use in the dashboard to visualize the HR attriton. Let load the data using file upload as per below.<br>
                            <img src="img/db1.png" alt="db1" style="width:800px"><br><br>

                            <u><b>Data Scrubbing</b></u><br>
                            Here, we don't have a method to scrub data as it is. We can only scrub data at the source csv. However we can filter here by unchecking blank input in the chart if there is.<br><br>
                            <u><b>Explore Data</b></u>
                            <u><b>Visualization Data as Scorecard</b></u><br>
                            In dashboard, i would like to add a few scorecard as shown below. Click on Add Chart and choose scorecard. You need to assign them metric by dragging related columns that you wish to show. 
                            Then you can use Count metric to show value. This dragging method is used to select data for each chart.
                            <br><img src="img/db2.png" alt="db2"><br><br>
                            <u><b>Visualization Data as Time Series Chart</b></u><br>
                            Here, I used time series to show performance of the staff and its numbers. We can how many performers and under performers.<br>
                            <img src="img/db3.png" alt="db3"><br><br>
                            <u><b>Visualization Data as Treemap</b></u><br>
                            I use Treemap for showing salary range. Treemap is type of data visualization that is especially useful for displaying hierarchical data.<br>	
                            <br><img src="img/db4.png" alt="db4"><br><br>
                            <u><b>Visualization Data as Pie Chart</b></u><br>
                            Another type chart, the pie chart. Here i use it to show how many percentage staff resign and staff based on the 15,0001 
                            <br><img src="img/db5.png" alt="db5"><br><br>
                            <u><b>Visualization Data as Bar Chart</b></u><br>
                            Here, I used Bar Chart to show 3 things<br>
                            1. Numbers of Staffs according to roles.<br>
                            2. Number of projects handled by staff that left & stay<br>
                            3. Number of projects handled by staff that left & stay  
                            <br><img src="img/db6.png" alt="db6"><br>
                            <br><img src="img/db7.png" alt="db7"><br>
                            <br><img src="img/db8.png" alt="db8"><br><br>
                            <u><b>Interpreting Data</b></u><br><br>
                                      <u>Findings from Dashboard</u><br>
                                      1. Staffs with more projects more than 4 tend to left shows overworking tend to force staff to resign..<br>
                                      2. In the same time, staffs with only 1 projects also left which mean underwork also a problem.<br>
                                      3. Only a handful staffs stay after 3 years, which show a high staff turnover.<br><br>

                            <u>Sugegestion to the HR Department.</u><br>
                            1. Set appropriate number of projects to be assigned to the staff.<br>
                            2. Give promotions accordingly to their performance.<br>
                            3. Salary need to be streamlined in the company, since there is huge gap between the number of small and medium/high salary earners.



                            
                            
                            
                            <br><br><br><br><br><br><br>
                            </p>
                            </div>
                            </div></div></div>
                            <div class="tm-section-wrap bg-white">
                            <section id="about" class="row tm-section">
                            <div class="col-12 tm-section-pad">
                            <div class="tm-flex-item-left">
                            <div class="tm-w-80">
                            <h2 class="tm-color-primary tm-section-title mb-4">About Me</h2>
                            <p class="mb-5">
                            <img src="img/khairudinprofile.jpeg" style="float:left;" alt="Profile Picture Khairudin" class="col-xs-12 col-sm-3">
                            HI! I am Khairudin. Currently, I’m a business development executive. For the past 14 years, I’ve been in roles where I assist 
                            and manage different type of projects in Finance and EPCC from start to finish. I’m well-versed in planning and 
                            development, helping clients to achieve their business needs and finalising financial reporting in group and company level. 
                            To further upskill myself, I recently completed IBM Data Analyst Professional Certificate and Data Science Uncut Bootcamp - LEAD. 
                            I look forward to continue to be involved in projects, either as a Data Analyst or Data Scientist.<br><br>
                            You can find my resume from the link <a href="https://drive.google.com/file/d/1WBPc2Rf6--t5z9qcUUu5cnW8CeS1mj9R/view?usp=sharing"target="_blank">here</a> for your consideration.<br><br>
                            Feel free to connect with me at kudin36@yahoo.com if you ever want to talk about technology, business, and career.<br>
                            
                          </p>
                          
                      </div>
          </div></div></div>
        </main>        
      </div>
    </div>
    <script src="js/jquery.min.js"></script>
    <script src="js/jquery.singlePageNav.min.js"></script>
    <script src="js/parallax.min.js"></script>
    <script src="slick/slick.min.js"></script>
    <script src="js/jquery.magnific-popup.min.js"></script>
    <script src="js/templatemo-scripts.js"></script>
  </body>
</html>